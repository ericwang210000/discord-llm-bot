QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.

The QwQ 32B model has the following features:
* Type: Causal Language Models
* Training Stage: Pretraining & Post-training (Supervised Finetuning and Reinforcement Learning)
* Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias
* Number of Parameters: 32.5B
* Number of Paramaters (Non-Embedding): 31.0B
* Number of Layers: 64
* Number of Attention Heads (GQA): 40 for Q and 8 for KV
* Context Length: Full 131,072 tokens